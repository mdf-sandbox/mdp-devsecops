name: reusable-workflow-deploy-ing

on:
  workflow_call:
    inputs:
      environment:
        type: string
        description: Target environment
        required: true
      repository:
        type: string
        description: Repository name
        required: true
      ref:
        type: string
        description: Branch name
        required: true
      baseline_no:
        type: string
        description: Deployment Baseline Number
        required: true
      post_deployment_script:
        type: string
        description: Post-deployment Scripts
        required: true
      run_id:
        type: string
        description: Databricks Run ID
        required: true

  # Allows you to run this workflow manually from the Action tab
  workflow_dispatch:
    inputs:
      environment:
        type: choice
        description: Target environment
        options:
         - uat
         - prod
      repository:
        type: string
        description: Repository name
        required: true
      ref:
        type: string
        description: Release tag
        required: true
      baseline_no:
        type: string
        description: Deployment Baseline Number
        required: true
      post_deployment_script:
        type: string
        description: Post-deployment Scripts
        required: true
      run_id:
        type: string
        description: Databricks Run ID
        required: true

# Use the Bash shell as default settings for all jobs in the workflow
defaults:
  run:
    shell: bash

jobs:
  # 
  validate-deploy:
    name: Validate Deployment Objects
    runs-on: ubuntu-20.04
   # needs: [deploy-databricks-bundle, deploy-files-to-volumes]     
    steps:
      - name: Checkout main branch of DevSecOps repository
        uses: actions/checkout@v4
        with:
          repository: ${{ vars.GHE_DEVOPS_REPOSITORY_VAR }}
          ref: main
          fetch-depth: 0

      - name: Load environment variables from ${{ inputs.environment }} dotenv file
        run: |
          cat ./dotenv/${{ inputs.environment }}.env >> $GITHUB_ENV

      - name: Checkout ${{ inputs.ref }} of ${{ inputs.repository }} repository
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.repository }}
          ref: ${{ inputs.ref }}
          fetch-depth: 0
          token: ${{ secrets.GHE_DEVOPS_TOKEN }}

      - name: Load environment variables from cicd env file
        run: |
          cat ./deployment/service/azure_devops/azure_devops_boards.env >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ vars.PYTHON_VERSION_VAR }}

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ vars.POETRY_VERSION_VAR }}

      - name: Install pip and etc
        run: |
          pip install --upgrade pip
          pip install jq
          pip install yq

      - name: Install AzCopy
        run: |
          wget -O azcopy_v10.tar.gz https://aka.ms/downloadazcopy-v10-linux && tar -xf azcopy_v10.tar.gz --strip-components=1
      
      - name: Set environment variables
        run: |
          echo "DEFAULT_WORKFLOW_NAME=$(echo ${{ inputs.repository }} | awk -F'/' '{print $2}')" >> $GITHUB_ENV
          echo "REPOSITORY=${{ inputs.repository }}" >> $GITHUB_ENV
          echo "APPROVAL_GATE_PATH=/Shared/MDF/${{ inputs.repository }}/${{ inputs.environment }}" >> $GITHUB_ENV
          echo "SCRIPTS_PATH=/Shared/MDF/${{ inputs.repository }}/scripts" >> $GITHUB_ENV
          echo "PACKAGE_NAME=$(echo ${{ inputs.repository }} | awk -F'/' '{print $2}')" >> $GITHUB_ENV
          echo "CURRENT_VERSION=$(poetry version --dry-run -s)" >> $GITHUB_ENV
          echo "ADLS_ARTIFACT_PATH=$(yq '.file_location_config' ./deployment/service/databricks/config.yml | jq -c '.adls_artifact | to_entries')" >> $GITHUB_ENV
          echo "AZCOPY_AUTO_LOGIN_TYPE=SPN" >> $GITHUB_ENV
          echo "AZCOPY_SPA_APPLICATION_ID=64443cb0-2542-4e10-90e2-f04295538d58" >> $GITHUB_ENV
          echo "AZCOPY_SPA_CLIENT_SECRET=${{ secrets.AZCOPY_SPA_CLIENT_SECRET }}" >> $GITHUB_ENV
          echo "AZCOPY_TENANT_ID=2aa3892b-2638-4281-a7e3-9623e880e79e" >> $GITHUB_ENV

      - name: Install Databricks CLI
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh

      - name: Azure login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AR_DEVOPS_CLIENT_ID }}","clientSecret":"${{ secrets.AR_DEVOPS_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AR_DEVOPS_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AR_DEVOPS_TENANT_ID }}"}'
          allow-no-subscriptions: true

      - name: Retrive Secrets from Azure Key Vault
        run: |
          echo "AR_FRMWK_CLIENT_ID_KV_VAL=$(az keyvault secret show --name '${{ env.AR_FRMWK_CLIENT_ID_KV }}' --vault-name '${{ env.AZURE_KEYVAULT_NAME }}' --query 'value' | jq -r)" >> $GITHUB_ENV
          echo "AR_FRMWK_CLIENT_SECRET_KV_VAL=$(az keyvault secret show --name '${{ env.AR_FRMWK_CLIENT_SECRET_KV }}' --vault-name '${{ env.AZURE_KEYVAULT_NAME }}' --query 'value' | jq -r)" >> $GITHUB_ENV
          echo "AR_FRMWK_SUBSCRIPTION_ID_KV_VAL=$(az keyvault secret show --name '${{ env.AR_DEVOPS_SUBSCRIPTION_ID_KV }}' --vault-name '${{ env.AZURE_KEYVAULT_NAME }}' --query 'value' | jq -r)" >> $GITHUB_ENV
          echo "AR_FRMWK_TENANT_ID_KV_VAL=$(az keyvault secret show --name '${{ env.AR_FRMWK_TENANT_ID_KV }}' --vault-name '${{ env.AZURE_KEYVAULT_NAME }}' --query 'value' | jq -r)" >> $GITHUB_ENV

      - name: Create Azure Databricks configuration profile
        run: |
          echo "[${{ env.DATABRICKS_PROFILE }}]" > ~/.databrickscfg
          echo "host                = ${{ env.DATABRICKS_HOST }}" >> ~/.databrickscfg
          echo "azure_tenant_id     = ${{ env.AR_FRMWK_TENANT_ID_KV_VAL }}" >> ~/.databrickscfg
          echo "azure_client_id     = ${{ env.AR_FRMWK_CLIENT_ID_KV_VAL }}" >> ~/.databrickscfg
          echo "azure_client_secret = ${{ env.AR_FRMWK_CLIENT_SECRET_KV_VAL }}" >> ~/.databrickscfg
          cat ~/.databrickscfg
      
      - name: Retrieve file counts from repository
        env:
          DDL_DB_INIT_PERSIST: "ddl/mdp/db_init/migration_test/persist"
          DDL_DB_INIT_RAW: "ddl/mdp/db_init/migration_test/raw"
          DDL_TABLE_INIT_PERSIST: "ddl/mdp/table_init/migration_test/persist"
          DDL_TABLE_INIT_RAW: "ddl/mdp/table_init/migration_test/raw"
          DDL_VIEW_INIT: "ddl/mdp/view_init/migration_test"
          VOL_CONFIG: "config/mdp/ingestion/migration_test"
          VOL_MAPPING: "mapping/mdp/ingestion_spec/migration_test"
        run: | 
          [ $(ls ${{ env.DDL_DB_INIT_PERSIST }} | sed 1d | wc -l) -eq $(databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_DB_INIT_PERSIST }} | sed 1d | wc -l) ] && echo "${{ env.DDL_DB_INIT_PERSIST }} matched!" || { echo "::error:: ${{ env.DDL_DB_INIT_PERSIST }} not matched!"; exit 1; }
          [ $(ls ${{ env.DDL_DB_INIT_RAW }} | wc -l) -eq $(databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_DB_INIT_RAW }} | sed 1d | wc -l) ] && echo "${{ env.DDL_DB_INIT_RAW }} matched!" || { echo "${{ env.DDL_DB_INIT_RAW }} not matched!"; exit 1; }
          
          [ $(ls ${{ env.DDL_TABLE_INIT_PERSIST }} | wc -l) -eq $(databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_TABLE_INIT_PERSIST }} | sed 1d | wc -l) ] && echo "${{ env.DDL_TABLE_INIT_PERSIST }} matched!" || { echo "${{ env.DDL_TABLE_INIT_PERSIST }} not matched!"; exit 1; }
          [ $(ls ${{ env.DDL_TABLE_INIT_RAW }} | wc -l) -eq $(databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_TABLE_INIT_RAW }} | sed 1d | wc -l) ] && echo "${{ env.DDL_TABLE_INIT_RAW }} matched!" || { echo "${{ env.DDL_TABLE_INIT_RAW }} not matched!"; exit 1; }

          [ $(ls ${{ env.DDL_VIEW_INIT }} | wc -l) -eq $(databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_VIEW_INIT }} | sed 1d | wc -l) ] && echo "${{ env.DDL_VIEW_INIT }} matched!" || { echo "${{ env.DDL_VIEW_INIT }} not matched!"; exit 1; }
          
          ls ${{ env.DDL_DB_INIT_PERSIST }} | wc -l
          databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_DB_INIT_PERSIST }} | sed 1d | wc -l
          ls ${{ env.DDL_DB_INIT_RAW }} | wc -l
          databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_DB_INIT_RAW }} | sed 1d | wc -l
          ls ${{ env.DDL_TABLE_INIT_PERSIST }} | wc -l
          databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_TABLE_INIT_PERSIST }} | sed 1d | wc -l
          ls ${{ env.DDL_TABLE_INIT_RAW }} | wc -l
          databricks --profile ${{ env.DATABRICKS_PROFILE }} workspace list /${{ env.DDL_TABLE_INIT_RAW }} | sed 1d | wc -l
          ls ${{ env.VOL_CONFIG }} | wc -l
          ls ${{ env.VOL_MAPPING }} | wc -l
          [ $(ls ${{ env.VOL_CONFIG }} | wc -l) -eq $(($(azcopy list "https://pocadlsmdf.dfs.core.windows.net/artifact/mdf/ingestion/test_deployment/${{ env.VOL_CONFIG }}" --running-tally | grep 'File count:' | awk '{print $4}') -1)) ] && echo "${{ env.VOL_CONFIG }} matched!" || { echo "${{ env.VOL_CONFIG }} not matched!"; exit 1; }
          [ $(ls ${{ env.VOL_MAPPING }} | wc -l) -eq $(($(azcopy list "https://pocadlsmdf.dfs.core.windows.net/artifact/mdf/ingestion/test_deployment/${{ env.VOL_MAPPING }}" --running-tally | grep 'File count:' | awk '{print $4}') -1)) ] && echo "${{ env.VOL_MAPPING }} matched!" || { echo "${{ env.VOL_MAPPING }} not matched!"; exit 1; }



